{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQ+3DagqXpvdk9cEav0cff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeGaYeun/Intel/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxbT4qkL42WG",
        "outputId": "79eb8fdf-94de-4d17-a5ce-1632850da108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflite-runtime\n",
            "  Downloading tflite_runtime-2.14.0-cp310-cp310-manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tflite-runtime) (1.23.5)\n",
            "Installing collected packages: tflite-runtime\n",
            "Successfully installed tflite-runtime-2.14.0\n"
          ]
        }
      ],
      "source": [
        "!python -m  pip install tflite-runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# 케라스 허브에서 모델 가져오기\n",
        "# https://www.kaggle.com/\n",
        "# mobilenet 검색 (_ v2)\n",
        "################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "print(metadata)\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "\n",
        "####################\n",
        "#케라스 허브에서 모델 가져오기\n",
        "###################\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://www.kaggle.com/models/google/mobilenet-v2/frameworks/TensorFlow2/variations/tf2-preview-feature-vector/versions/4\",\n",
        "                   input_shape = (224,224,3),\n",
        "                   output_shape=[1280],\n",
        "                   trainable=False),  # Can be True, see below.\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "kmnist_SAVED_MODEL = \"/content/drive/MyDrive/INTEL_PYTHON/exp_saved_model\"\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "#tensoflow lite로 변환하기\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "tflite_model = converter.convert()\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/converted_model.tflite'\n",
        "\n",
        "#with open(…) as f 에서 f는 open(…)함수가 리턴한 file object.\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "print('input: ', input_details)\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "print('output: ', output_details)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC8aWbO--klj",
        "outputId": "0ff751a4-02a2-4ffb-be79-8b668562d29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='kmnist',\n",
            "    full_name='kmnist/3.0.1',\n",
            "    description=\"\"\"\n",
            "    Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images), provided in the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes, we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.\n",
            "    \"\"\",\n",
            "    homepage='http://codh.rois.ac.jp/kmnist/index.html.en',\n",
            "    data_dir='/root/tensorflow_datasets/kmnist/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=20.26 MiB,\n",
            "    dataset_size=31.76 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@online{clanuwat2018deep,\n",
            "      author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha},\n",
            "      title        = {Deep Learning for Classical Japanese Literature},\n",
            "      date         = {2018-12-03},\n",
            "      year         = {2018},\n",
            "      eprintclass  = {cs.CV},\n",
            "      eprinttype   = {arXiv},\n",
            "      eprint       = {cs.CV/1812.01718},\n",
            "    }\"\"\",\n",
            ")\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 71s 42ms/step - loss: 0.5720 - accuracy: 0.8329 - val_loss: 0.3694 - val_accuracy: 0.8895\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 59s 39ms/step - loss: 0.3148 - accuracy: 0.9082 - val_loss: 0.2890 - val_accuracy: 0.9127\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2565 - accuracy: 0.9232 - val_loss: 0.2605 - val_accuracy: 0.9227\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 66s 44ms/step - loss: 0.2246 - accuracy: 0.9332 - val_loss: 0.2523 - val_accuracy: 0.9227\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 65s 43ms/step - loss: 0.2047 - accuracy: 0.9383 - val_loss: 0.2326 - val_accuracy: 0.9312\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_3 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2270794 (8.66 MB)\n",
            "Trainable params: 12810 (50.04 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n",
            "input:  {'name': 'serving_default_keras_layer_3_input:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "output:  {'name': 'StatefulPartitionedCall:0', 'index': 177, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "R36SjqDtWEZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "#.tflite로 변환후 추론 테스트\n",
        "################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "#examples.batch(20).prefetch(1) will prefetch 1 elements (2 batches, of 20 examples each).\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "#텐서플로우 허브에 있는 mobilenet_v2 모델을 사용해 feature_extractor라는 케라스 층을 만듭니다.\n",
        "#신경망의 첫번째 층\n",
        "handle_base, pixels, FV_SIZE = (\"mobilenet_v2\", 224, 1280)\n",
        "\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "\n",
        "IMAGE_SIZE =(pixels, pixels)\n",
        "\n",
        "\n",
        "feature_extractor = hub.KerasLayer(\n",
        "    MODULE_HANDLE,\n",
        "    input_shape = IMAGE_SIZE +(3,),\n",
        "    output_shape = [FV_SIZE],\n",
        "    trainable = False\n",
        ")\n",
        "\n",
        "# 모델 선언\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dense(num_classes, activation ='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "#colab 버전\n",
        "kmnist_SAVED_MODEL = \"/content/drive/MyDrive/INTEL_PYTHON/exp_saved_model\"\n",
        "#pc 버전\n",
        "#CATS_VS_DOGS_SAVED_MODEL = 'exp_saved_model'\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "\n",
        "#tensoflow lite로 변환하기\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "tflite_model = converter.convert()\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/converted_model.tflite'\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "print('input: ', input_details)\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "print('output: ', output_details)\n",
        "\n",
        "\n",
        "#################추론 테스트######################\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "#테스트 배치 파일을 10개 씩 가져와서\n",
        "predictions =[]\n",
        "test_labels, test_imgs = [],[]\n",
        "for img, label in test_batches.take(10):\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions.append(interpreter.get_tensor(output_index))\n",
        "    test_labels.append(label.numpy()[0])\n",
        "    test_imgs.append(img)\n",
        "\n",
        "print(\"predictions:\",predictions)\n",
        "print(\"labels:\",test_labels)\n",
        "\n",
        "#예측 수 비교\n",
        "score  = 0\n",
        "for item in range(0,10):\n",
        "    prediction = np.argmax(predictions[item])\n",
        "    label = test_labels[item]\n",
        "    if prediction == label:\n",
        "        score = score +1\n",
        "\n",
        "\n",
        "print(\"10개 중 맞은 예측 수: \"+ str(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8LpuTgwG6B9",
        "outputId": "9eccbd6e-8128-41e5-959b-0163b58e8179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 76s 48ms/step - loss: 0.5729 - accuracy: 0.8318 - val_loss: 0.3446 - val_accuracy: 0.8988\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 59s 39ms/step - loss: 0.3123 - accuracy: 0.9096 - val_loss: 0.2834 - val_accuracy: 0.9145\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2542 - accuracy: 0.9251 - val_loss: 0.2691 - val_accuracy: 0.9182\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2242 - accuracy: 0.9340 - val_loss: 0.2622 - val_accuracy: 0.9210\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 59s 39ms/step - loss: 0.2024 - accuracy: 0.9391 - val_loss: 0.2390 - val_accuracy: 0.9260\n",
            "input:  {'name': 'serving_default_keras_layer_5_input:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "output:  {'name': 'StatefulPartitionedCall:0', 'index': 177, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "predictions: [array([[1.4798417e-04, 1.3333090e-03, 1.1845635e-02, 9.8511249e-01,\n",
            "        8.6139952e-04, 2.3072239e-04, 1.3667692e-05, 3.9477703e-07,\n",
            "        4.3801352e-04, 1.6405735e-05]], dtype=float32), array([[1.9222264e-06, 9.9822956e-01, 1.0091990e-04, 4.9436844e-06,\n",
            "        9.7857055e-04, 3.8497712e-04, 3.4910111e-06, 7.1444487e-07,\n",
            "        2.8993174e-05, 2.6580485e-04]], dtype=float32), array([[1.7235046e-06, 8.9502166e-08, 2.2470894e-08, 2.5107387e-11,\n",
            "        1.1059680e-06, 3.4764702e-09, 5.3277873e-08, 9.9999702e-01,\n",
            "        6.7036254e-10, 2.4334013e-09]], dtype=float32), array([[8.7191358e-02, 1.1967696e-02, 2.5681376e-03, 5.4636924e-03,\n",
            "        6.4201367e-01, 7.2861478e-02, 1.1170871e-01, 1.6523123e-02,\n",
            "        4.9184606e-04, 4.9210273e-02]], dtype=float32), array([[1.2596432e-05, 3.4641096e-05, 1.4369673e-05, 4.2162351e-06,\n",
            "        4.1097570e-05, 4.0813279e-04, 2.3885008e-05, 2.2297548e-03,\n",
            "        6.9257389e-05, 9.9716210e-01]], dtype=float32), array([[6.0656583e-03, 3.5588900e-03, 9.1729283e-01, 8.9847209e-04,\n",
            "        1.0425611e-02, 3.5079939e-03, 1.6510302e-03, 6.7198282e-04,\n",
            "        2.2697108e-04, 5.5700462e-02]], dtype=float32), array([[9.4108933e-01, 3.3880796e-03, 9.4877600e-05, 1.1628079e-02,\n",
            "        3.6664560e-02, 1.9207221e-03, 3.2793391e-03, 9.9195947e-04,\n",
            "        4.5940475e-04, 4.8352772e-04]], dtype=float32), array([[7.8994187e-04, 2.7817523e-03, 2.0553302e-02, 4.9346310e-01,\n",
            "        7.5732882e-04, 6.4350385e-03, 2.0492582e-03, 5.8349210e-06,\n",
            "        4.7098923e-01, 2.1752105e-03]], dtype=float32), array([[5.55192455e-05, 5.47105483e-05, 1.09191351e-05, 9.97150362e-01,\n",
            "        1.20540055e-04, 4.51063417e-04, 4.69272436e-06, 2.12795264e-03,\n",
            "        2.40703121e-05, 1.23007126e-07]], dtype=float32), array([[3.2802478e-03, 8.7646376e-03, 5.3112197e-04, 8.2079036e-04,\n",
            "        5.5398945e-02, 1.7270599e-02, 5.9541124e-03, 8.2586696e-03,\n",
            "        1.2074010e-02, 8.8764691e-01]], dtype=float32)]\n",
            "labels: [3, 1, 7, 4, 9, 2, 0, 3, 3, 9]\n",
            "10개 중 맞은 예측 수: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# 2-2. 전이학습+최적화(동적 범위 양자화 (dynamic range quantization))\n",
        "# + tflite로 변환\n",
        "################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "#텐서플로우 허브에 있는 mobilenet_v2 모델을 사용해 feature_extractor라는 케라스 층을 만듭니다.\n",
        "#신경망의 첫번째 층\n",
        "handle_base, pixels, FV_SIZE = (\"mobilenet_v2\", 224, 1280)\n",
        "\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "\n",
        "IMAGE_SIZE =(pixels, pixels)\n",
        "\n",
        "\n",
        "feature_extractor = hub.KerasLayer(\n",
        "    MODULE_HANDLE,\n",
        "    input_shape = IMAGE_SIZE +(3,), #(224,224,3)Expects a tensor of shape [batch_size] as input.\n",
        "    output_shape = [FV_SIZE],# Outputs a tensor with shape [batch_size, 20].\n",
        "    trainable = False\n",
        ")\n",
        "\n",
        "# 모델 선언\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dense(num_classes, activation ='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "#colab 버전\n",
        "kmnist_SAVED_MODEL = '/content/drive/MyDrive/INTEL_PYTHON/drq_exp_saved_model'\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "\n",
        "#tensoflow lite로 변환하기\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/drq_converted_model.tflite'\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "print('input: ', input_details)\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "print('output: ', output_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EEtFKusIA3c",
        "outputId": "720a7f26-cebf-420c-ce2d-a3910bb8dfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 64s 40ms/step - loss: 0.5818 - accuracy: 0.8278 - val_loss: 0.3544 - val_accuracy: 0.8953\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 62s 41ms/step - loss: 0.3137 - accuracy: 0.9086 - val_loss: 0.2859 - val_accuracy: 0.9125\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 62s 41ms/step - loss: 0.2568 - accuracy: 0.9249 - val_loss: 0.2586 - val_accuracy: 0.9205\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 58s 39ms/step - loss: 0.2239 - accuracy: 0.9340 - val_loss: 0.2495 - val_accuracy: 0.9238\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2026 - accuracy: 0.9388 - val_loss: 0.2211 - val_accuracy: 0.9352\n",
            "input:  {'name': 'serving_default_keras_layer_7_input:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "output:  {'name': 'StatefulPartitionedCall:0', 'index': 177, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# 2-2.전이학습+최적화(DRQ) + tflite로 변환\n",
        "# (추가) tf.lite.experimental.Analyzer.analyze 결과 보기\n",
        "################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "#텐서플로우 허브에 있는 mobilenet_v2 모델을 사용해 feature_extractor라는 케라스 층을 만듭니다.\n",
        "#신경망의 첫번째 층\n",
        "handle_base, pixels, FV_SIZE = (\"mobilenet_v2\", 224, 1280)\n",
        "\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "\n",
        "IMAGE_SIZE =(pixels, pixels)\n",
        "\n",
        "\n",
        "feature_extractor = hub.KerasLayer(\n",
        "    MODULE_HANDLE,\n",
        "    input_shape = IMAGE_SIZE +(3,),\n",
        "    output_shape = [FV_SIZE],\n",
        "    trainable = False\n",
        ")\n",
        "\n",
        "# 모델 선언\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dense(num_classes, activation ='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "kmnist_SAVED_MODEL = '/content/drive/MyDrive/INTEL_PYTHON/exp_saved_model'\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "\n",
        "#tensoflow lite로 변환하기\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)\n",
        "\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/drq_converted_model.tflite'\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qxum1WmMjaw",
        "outputId": "a1c3494e-562e-4ffc-8258-cdb1b2bb852c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_8 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2270794 (8.66 MB)\n",
            "Trainable params: 12810 (50.04 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 64s 40ms/step - loss: 0.5787 - accuracy: 0.8270 - val_loss: 0.3542 - val_accuracy: 0.8965\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 62s 42ms/step - loss: 0.3157 - accuracy: 0.9070 - val_loss: 0.2951 - val_accuracy: 0.9107\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 65s 43ms/step - loss: 0.2570 - accuracy: 0.9237 - val_loss: 0.2739 - val_accuracy: 0.9158\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2249 - accuracy: 0.9327 - val_loss: 0.2424 - val_accuracy: 0.9257\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 58s 39ms/step - loss: 0.2043 - accuracy: 0.9382 - val_loss: 0.2272 - val_accuracy: 0.9305\n",
            "=== TFLite ModelAnalyzer ===\n",
            "\n",
            "Your TFLite model has '1' subgraph(s). In the subgraph description below,\n",
            "T# represents the Tensor numbers. For example, in Subgraph#0, the MUL op takes\n",
            "tensor #0 and tensor #1 as input and produces tensor #110 as output.\n",
            "\n",
            "Subgraph#0 main(T#0) -> [T#177]\n",
            "  Op#0 MUL(T#0, T#1) -> [T#110]\n",
            "  Op#1 SUB(T#110, T#2) -> [T#111]\n",
            "  Op#2 CONV_2D(T#111, T#5, T#22) -> [T#112]\n",
            "  Op#3 DEPTHWISE_CONV_2D(T#112, T#57, T#23) -> [T#113]\n",
            "  Op#4 CONV_2D(T#113, T#4, T#58) -> [T#114]\n",
            "  Op#5 CONV_2D(T#114, T#76, T#24) -> [T#115]\n",
            "  Op#6 DEPTHWISE_CONV_2D(T#115, T#59, T#25) -> [T#116]\n",
            "  Op#7 CONV_2D(T#116, T#77, T#60) -> [T#117]\n",
            "  Op#8 CONV_2D(T#117, T#78, T#26) -> [T#118]\n",
            "  Op#9 DEPTHWISE_CONV_2D(T#118, T#61, T#27) -> [T#119]\n",
            "  Op#10 CONV_2D(T#119, T#79, T#21) -> [T#120]\n",
            "  Op#11 ADD(T#120, T#117) -> [T#121]\n",
            "  Op#12 CONV_2D(T#121, T#80, T#28) -> [T#122]\n",
            "  Op#13 DEPTHWISE_CONV_2D(T#122, T#62, T#29) -> [T#123]\n",
            "  Op#14 CONV_2D(T#123, T#81, T#20) -> [T#124]\n",
            "  Op#15 CONV_2D(T#124, T#82, T#30) -> [T#125]\n",
            "  Op#16 DEPTHWISE_CONV_2D(T#125, T#63, T#31) -> [T#126]\n",
            "  Op#17 CONV_2D(T#126, T#83, T#19) -> [T#127]\n",
            "  Op#18 ADD(T#127, T#124) -> [T#128]\n",
            "  Op#19 CONV_2D(T#128, T#84, T#32) -> [T#129]\n",
            "  Op#20 DEPTHWISE_CONV_2D(T#129, T#64, T#33) -> [T#130]\n",
            "  Op#21 CONV_2D(T#130, T#85, T#18) -> [T#131]\n",
            "  Op#22 ADD(T#131, T#128) -> [T#132]\n",
            "  Op#23 CONV_2D(T#132, T#86, T#34) -> [T#133]\n",
            "  Op#24 DEPTHWISE_CONV_2D(T#133, T#65, T#35) -> [T#134]\n",
            "  Op#25 CONV_2D(T#134, T#87, T#17) -> [T#135]\n",
            "  Op#26 CONV_2D(T#135, T#88, T#36) -> [T#136]\n",
            "  Op#27 DEPTHWISE_CONV_2D(T#136, T#66, T#37) -> [T#137]\n",
            "  Op#28 CONV_2D(T#137, T#89, T#16) -> [T#138]\n",
            "  Op#29 ADD(T#138, T#135) -> [T#139]\n",
            "  Op#30 CONV_2D(T#139, T#90, T#38) -> [T#140]\n",
            "  Op#31 DEPTHWISE_CONV_2D(T#140, T#67, T#39) -> [T#141]\n",
            "  Op#32 CONV_2D(T#141, T#91, T#15) -> [T#142]\n",
            "  Op#33 ADD(T#142, T#139) -> [T#143]\n",
            "  Op#34 CONV_2D(T#143, T#92, T#40) -> [T#144]\n",
            "  Op#35 DEPTHWISE_CONV_2D(T#144, T#68, T#41) -> [T#145]\n",
            "  Op#36 CONV_2D(T#145, T#93, T#14) -> [T#146]\n",
            "  Op#37 ADD(T#146, T#143) -> [T#147]\n",
            "  Op#38 CONV_2D(T#147, T#94, T#42) -> [T#148]\n",
            "  Op#39 DEPTHWISE_CONV_2D(T#148, T#69, T#43) -> [T#149]\n",
            "  Op#40 CONV_2D(T#149, T#95, T#13) -> [T#150]\n",
            "  Op#41 CONV_2D(T#150, T#96, T#44) -> [T#151]\n",
            "  Op#42 DEPTHWISE_CONV_2D(T#151, T#70, T#45) -> [T#152]\n",
            "  Op#43 CONV_2D(T#152, T#97, T#12) -> [T#153]\n",
            "  Op#44 ADD(T#153, T#150) -> [T#154]\n",
            "  Op#45 CONV_2D(T#154, T#98, T#46) -> [T#155]\n",
            "  Op#46 DEPTHWISE_CONV_2D(T#155, T#71, T#47) -> [T#156]\n",
            "  Op#47 CONV_2D(T#156, T#99, T#11) -> [T#157]\n",
            "  Op#48 ADD(T#157, T#154) -> [T#158]\n",
            "  Op#49 CONV_2D(T#158, T#100, T#48) -> [T#159]\n",
            "  Op#50 DEPTHWISE_CONV_2D(T#159, T#72, T#49) -> [T#160]\n",
            "  Op#51 CONV_2D(T#160, T#101, T#10) -> [T#161]\n",
            "  Op#52 CONV_2D(T#161, T#102, T#50) -> [T#162]\n",
            "  Op#53 DEPTHWISE_CONV_2D(T#162, T#73, T#51) -> [T#163]\n",
            "  Op#54 CONV_2D(T#163, T#103, T#9) -> [T#164]\n",
            "  Op#55 ADD(T#164, T#161) -> [T#165]\n",
            "  Op#56 CONV_2D(T#165, T#104, T#52) -> [T#166]\n",
            "  Op#57 DEPTHWISE_CONV_2D(T#166, T#74, T#53) -> [T#167]\n",
            "  Op#58 CONV_2D(T#167, T#105, T#8) -> [T#168]\n",
            "  Op#59 ADD(T#168, T#165) -> [T#169]\n",
            "  Op#60 CONV_2D(T#169, T#106, T#54) -> [T#170]\n",
            "  Op#61 DEPTHWISE_CONV_2D(T#170, T#75, T#55) -> [T#171]\n",
            "  Op#62 CONV_2D(T#171, T#107, T#7) -> [T#172]\n",
            "  Op#63 CONV_2D(T#172, T#108, T#56) -> [T#173]\n",
            "  Op#64 AVERAGE_POOL_2D(T#173) -> [T#174]\n",
            "  Op#65 RESHAPE(T#174, T#6[-1, 1280]) -> [T#175]\n",
            "  Op#66 FULLY_CONNECTED(T#175, T#109, T#3) -> [T#176]\n",
            "  Op#67 SOFTMAX(T#176) -> [T#177]\n",
            "\n",
            "Tensors of Subgraph#0\n",
            "  T#0(serving_default_keras_layer_8_input:0) shape_signature:[-1, 224, 224, 3], type:FLOAT32\n",
            "  T#1(predict/hub_input/Mul/y) shape:[], type:FLOAT32 RO 4 bytes, buffer: 2, data:[2]\n",
            "  T#2(predict/hub_input/Sub/y) shape:[], type:FLOAT32 RO 4 bytes, buffer: 3, data:[1]\n",
            "  T#3(sequential_10/dense_10/BiasAdd/ReadVariableOp) shape:[10], type:FLOAT32 RO 40 bytes, buffer: 4, data:[-0.0585034, -0.0472996, -0.00579942, -0.015029, -0.00491601, ...]\n",
            "  T#4(predict/MobilenetV2/expanded_conv/project/Conv2D) shape:[16, 1, 1, 32], type:FLOAT32 RO 2048 bytes, buffer: 5, data:[-0.0626527, -0.423512, 0.0470475, -7.11342e-21, -0.115254, ...]\n",
            "  T#5(predict/MobilenetV2/Conv/Conv2D) shape:[32, 3, 3, 3], type:FLOAT32 RO 3456 bytes, buffer: 6, data:[-0.301457, -0.560106, -0.186869, -0.504564, -0.825698, ...]\n",
            "  T#6(predict/feature_vector/SpatialSqueeze) shape:[2], type:INT32 RO 8 bytes, buffer: 7, data:[-1, 1280]\n",
            "  T#7(predict/MobilenetV2/expanded_conv_16/project/BatchNorm/FusedBatchNorm) shape:[320], type:FLOAT32 RO 1280 bytes, buffer: 8, data:[-4.01373, 7.56115, 2.31113, -2.96572, -2.78331, ...]\n",
            "  T#8(predict/MobilenetV2/expanded_conv_15/project/BatchNorm/FusedBatchNorm) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 9, data:[-3.44666, 5.12284, 0.817276, -5.53406, -6.42678, ...]\n",
            "  T#9(predict/MobilenetV2/expanded_conv_14/project/BatchNorm/FusedBatchNorm) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 10, data:[2.62462, -0.794928, -0.23765, -1.07118, 1.3908, ...]\n",
            "  T#10(predict/MobilenetV2/expanded_conv_13/project/BatchNorm/FusedBatchNorm) shape:[160], type:FLOAT32 RO 640 bytes, buffer: 11, data:[-15.1578, 7.49733, 9.65496, -9.54922, -5.06211, ...]\n",
            "  T#11(predict/MobilenetV2/expanded_conv_12/project/BatchNorm/FusedBatchNorm) shape:[96], type:FLOAT32 RO 384 bytes, buffer: 12, data:[-6.95167, -3.93733, 1.67949, 0.788028, -3.7474, ...]\n",
            "  T#12(predict/MobilenetV2/expanded_conv_11/project/BatchNorm/FusedBatchNorm) shape:[96], type:FLOAT32 RO 384 bytes, buffer: 13, data:[0.706891, 0.723089, 0.388241, 1.5054, -4.64588, ...]\n",
            "  T#13(predict/MobilenetV2/expanded_conv_10/project/BatchNorm/FusedBatchNorm) shape:[96], type:FLOAT32 RO 384 bytes, buffer: 14, data:[1.87939, -0.284218, 1.4799, 4.94681, 1.23692, ...]\n",
            "  T#14(predict/MobilenetV2/expanded_conv_9/project/BatchNorm/FusedBatchNorm) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 15, data:[-0.302015, -0.836114, -0.919145, 0.991387, 0.362973, ...]\n",
            "  T#15(predict/MobilenetV2/expanded_conv_8/project/BatchNorm/FusedBatchNorm) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 16, data:[1.09699, 4.86088, 0.969056, -0.153911, -1.47135, ...]\n",
            "  T#16(predict/MobilenetV2/expanded_conv_7/project/BatchNorm/FusedBatchNorm) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 17, data:[0.855777, -0.989516, -0.282604, 0.482924, -0.0480442, ...]\n",
            "  T#17(predict/MobilenetV2/expanded_conv_6/project/BatchNorm/FusedBatchNorm) shape:[64], type:FLOAT32 RO 256 bytes, buffer: 18, data:[-13.8803, 0.936114, -2.38041, 7.05651, -0.0234004, ...]\n",
            "  T#18(predict/MobilenetV2/expanded_conv_5/project/BatchNorm/FusedBatchNorm) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 19, data:[2.48176, 0.373492, 4.07313, -1.83819, 3.16383, ...]\n",
            "  T#19(predict/MobilenetV2/expanded_conv_4/project/BatchNorm/FusedBatchNorm) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 20, data:[6.47212, -0.3994, 1.10535, 8.05952, 13.7698, ...]\n",
            "  T#20(predict/MobilenetV2/expanded_conv_3/project/BatchNorm/FusedBatchNorm) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 21, data:[-3.62612, 5.31772, 4.73441, -4.48393, 11.4146, ...]\n",
            "  T#21(predict/MobilenetV2/expanded_conv_2/project/BatchNorm/FusedBatchNorm) shape:[24], type:FLOAT32 RO 96 bytes, buffer: 22, data:[23.3477, -6.77157, 9.12497, -4.29681, 1.4932, ...]\n",
            "  T#22(predict/MobilenetV2/Conv/BatchNorm/FusedBatchNorm) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 23, data:[2.32011, 2.9058, -1.24403, -1.21565, 3.46697, ...]\n",
            "  T#23(predict/MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm) shape:[32], type:FLOAT32 RO 128 bytes, buffer: 24, data:[1.4337, 3.90283, -0.17387, -0.151181, 3.17039, ...]\n",
            "  T#24(predict/MobilenetV2/expanded_conv_1/expand/BatchNorm/FusedBatchNorm) shape:[96], type:FLOAT32 RO 384 bytes, buffer: 25, data:[0.804131, -0.00507668, 0.00250391, 2.56029, -0.131745, ...]\n",
            "  T#25(predict/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/FusedBatchNorm) shape:[96], type:FLOAT32 RO 384 bytes, buffer: 26, data:[0.0682313, 0.118235, 0.00101596, -0.832745, 0.140271, ...]\n",
            "  T#26(predict/MobilenetV2/expanded_conv_2/expand/BatchNorm/FusedBatchNorm) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 27, data:[0.854844, -1.33258, 0.854106, 0.250733, -0.36764, ...]\n",
            "  T#27(predict/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/FusedBatchNorm) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 28, data:[1.45485, -1.20594, -0.0203946, -1.32637, 0.413162, ...]\n",
            "  T#28(predict/MobilenetV2/expanded_conv_3/expand/BatchNorm/FusedBatchNorm) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 29, data:[-0.663317, 0.141478, -0.0139489, -0.183179, 0.439843, ...]\n",
            "  T#29(predict/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/FusedBatchNorm) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 30, data:[0.098287, 1.83491, 5.8866, 1.96369, 1.82132, ...]\n",
            "  T#30(predict/MobilenetV2/expanded_conv_4/expand/BatchNorm/FusedBatchNorm) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 31, data:[0.599366, 0.946022, 0.605991, 1.50515, 0.511877, ...]\n",
            "  T#31(predict/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/FusedBatchNorm) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 32, data:[0.00823942, -0.0328981, 0.925146, 0.0212408, -0.00302112, ...]\n",
            "  T#32(predict/MobilenetV2/expanded_conv_5/expand/BatchNorm/FusedBatchNorm) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 33, data:[-0.746838, 1.19281, -0.497792, 1.09965, 1.23324, ...]\n",
            "  T#33(predict/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/FusedBatchNorm) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 34, data:[-0.532422, 0.00270182, -1.45915, -0.428326, 7.54825, ...]\n",
            "  T#34(predict/MobilenetV2/expanded_conv_6/expand/BatchNorm/FusedBatchNorm) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 35, data:[-0.463865, 1.208, -0.524932, 0.394461, 0.521281, ...]\n",
            "  T#35(predict/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/FusedBatchNorm) shape:[192], type:FLOAT32 RO 768 bytes, buffer: 36, data:[4.76501, -0.224685, -0.01511, 3.3579, 0.00136673, ...]\n",
            "  T#36(predict/MobilenetV2/expanded_conv_7/expand/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 37, data:[-0.541272, 0.413853, 0.938305, 0.784311, 1.22395, ...]\n",
            "  T#37(predict/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 38, data:[-1.4303, -1.35161, -0.126759, -0.0496381, 5.81291, ...]\n",
            "  T#38(predict/MobilenetV2/expanded_conv_8/expand/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 39, data:[0.643296, 1.16173, -0.425155, 1.01049, 1.04765, ...]\n",
            "  T#39(predict/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 40, data:[-1.19466, 2.36424, -1.39457, 1.49498, 0.0494823, ...]\n",
            "  T#40(predict/MobilenetV2/expanded_conv_9/expand/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 41, data:[0.816238, -0.821796, -1.22857, 0.984121, -0.50019, ...]\n",
            "  T#41(predict/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 42, data:[-0.640067, 0.0242525, -0.0113323, -0.802542, -2.52369, ...]\n",
            "  T#42(predict/MobilenetV2/expanded_conv_10/expand/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 43, data:[0.708116, 1.0428, -0.751156, 1.22692, -0.13528, ...]\n",
            "  T#43(predict/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm) shape:[384], type:FLOAT32 RO 1536 bytes, buffer: 44, data:[2.1265, -1.86836, -0.16281, -2.71638, 0.87415, ...]\n",
            "  T#44(predict/MobilenetV2/expanded_conv_11/expand/BatchNorm/FusedBatchNorm) shape:[576], type:FLOAT32 RO 2304 bytes, buffer: 45, data:[-0.277402, -0.783054, -0.881437, -0.894695, -0.182604, ...]\n",
            "  T#45(predict/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm) shape:[576], type:FLOAT32 RO 2304 bytes, buffer: 46, data:[-1.50165, -0.221031, -0.85953, 0.36589, 1.39187, ...]\n",
            "  T#46(predict/MobilenetV2/expanded_conv_12/expand/BatchNorm/FusedBatchNorm) shape:[576], type:FLOAT32 RO 2304 bytes, buffer: 47, data:[0.800925, -0.895904, -0.143575, -0.821723, -1.154, ...]\n",
            "  T#47(predict/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm) shape:[576], type:FLOAT32 RO 2304 bytes, buffer: 48, data:[-0.92411, -1.10603, -1.33684, -1.8186, 1.08492, ...]\n",
            "  T#48(predict/MobilenetV2/expanded_conv_13/expand/BatchNorm/FusedBatchNorm) shape:[576], type:FLOAT32 RO 2304 bytes, buffer: 49, data:[-1.81365, -1.01515, -1.56788, -0.660075, -1.59289, ...]\n",
            "  T#49(predict/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm) shape:[576], type:FLOAT32 RO 2304 bytes, buffer: 50, data:[-0.0357365, -0.000196248, -0.00739066, 1.58657, -0.00379565, ...]\n",
            "  T#50(predict/MobilenetV2/expanded_conv_14/expand/BatchNorm/FusedBatchNorm) shape:[960], type:FLOAT32 RO 3840 bytes, buffer: 51, data:[0.620677, -1.00436, 0.674321, 0.958879, -0.198139, ...]\n",
            "  T#51(predict/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm) shape:[960], type:FLOAT32 RO 3840 bytes, buffer: 52, data:[-0.294111, -2.1462, -0.652545, -0.120927, 0.555778, ...]\n",
            "  T#52(predict/MobilenetV2/expanded_conv_15/expand/BatchNorm/FusedBatchNorm) shape:[960], type:FLOAT32 RO 3840 bytes, buffer: 53, data:[-0.5581, 1.06177, 0.222358, 0.718587, 0.943762, ...]\n",
            "  T#53(predict/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm) shape:[960], type:FLOAT32 RO 3840 bytes, buffer: 54, data:[-1.81494, -0.699074, -2.73506, -0.766427, -0.23984, ...]\n",
            "  T#54(predict/MobilenetV2/expanded_conv_16/expand/BatchNorm/FusedBatchNorm) shape:[960], type:FLOAT32 RO 3840 bytes, buffer: 55, data:[-1.28354, -1.01221, 0.346572, -0.907286, -0.953261, ...]\n",
            "  T#55(predict/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm) shape:[960], type:FLOAT32 RO 3840 bytes, buffer: 56, data:[0.0256587, 0.16207, -5.4283, 2.63806, -0.145296, ...]\n",
            "  T#56(predict/MobilenetV2/Conv_1/BatchNorm/FusedBatchNorm) shape:[1280], type:FLOAT32 RO 5120 bytes, buffer: 57, data:[-4.76913, -4.23232, -4.89652, -4.88836, -4.3217, ...]\n",
            "  T#57(predict/MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv/depthwise/depthwise;predict/MobilenetV2/expanded_conv_5/project/Conv2D) shape:[1, 3, 3, 32], type:FLOAT32 RO 1152 bytes, buffer: 58, data:[-0.100348, 0.0630403, -0.967416, -42.4027, -2.80899, ...]\n",
            "  T#58(predict/MobilenetV2/expanded_conv/project/BatchNorm/FusedBatchNorm) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 59, data:[-1.79617, 3.17504, 16.2981, 24.036, 13.8514, ...]\n",
            "  T#59(predict/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_1/depthwise/depthwise;predict/MobilenetV2/expanded_conv_12/project/Conv2D) shape:[1, 3, 3, 96], type:FLOAT32 RO 3456 bytes, buffer: 60, data:[-0.257105, 0.312097, 0.0698981, 0.238104, 0.113416, ...]\n",
            "  T#60(predict/MobilenetV2/expanded_conv_1/project/BatchNorm/FusedBatchNorm) shape:[24], type:FLOAT32 RO 96 bytes, buffer: 61, data:[8.0818, -7.41288, 14.1258, -8.88348, 1.68603, ...]\n",
            "  T#61(predict/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_2/depthwise/depthwise;predict/MobilenetV2/expanded_conv_3/depthwise/depthwise) shape:[1, 3, 3, 144], type:INT8 RO 1296 bytes, buffer: 62, data:[., ., m, ., ., ...]\n",
            "  T#62(predict/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_3/depthwise/depthwise) shape:[1, 3, 3, 144], type:INT8 RO 1296 bytes, buffer: 63, data:[F, ., ., ., ., ...]\n",
            "  T#63(predict/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_4/depthwise/depthwise;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise) shape:[1, 3, 3, 192], type:INT8 RO 1728 bytes, buffer: 64, data:[., #, ., #, ., ...]\n",
            "  T#64(predict/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_5/depthwise/depthwise;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise) shape:[1, 3, 3, 192], type:INT8 RO 1728 bytes, buffer: 65, data:[7, ., W, ., ., ...]\n",
            "  T#65(predict/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise) shape:[1, 3, 3, 192], type:INT8 RO 1728 bytes, buffer: 66, data:[., ., 6, ., -, ...]\n",
            "  T#66(predict/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_7/depthwise/depthwise;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise) shape:[1, 3, 3, 384], type:INT8 RO 3456 bytes, buffer: 67, data:[., &, ., ., ., ...]\n",
            "  T#67(predict/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_8/depthwise/depthwise;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise) shape:[1, 3, 3, 384], type:INT8 RO 3456 bytes, buffer: 68, data:[., ., ., ., ., ...]\n",
            "  T#68(predict/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_9/depthwise/depthwise;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise) shape:[1, 3, 3, 384], type:INT8 RO 3456 bytes, buffer: 69, data:[7, ., ), ., ., ...]\n",
            "  T#69(predict/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise) shape:[1, 3, 3, 384], type:INT8 RO 3456 bytes, buffer: 70, data:[., ., ., ., I, ...]\n",
            "  T#70(predict/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_11/depthwise/depthwise;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise) shape:[1, 3, 3, 576], type:INT8 RO 5184 bytes, buffer: 71, data:[#, ., ., ., ., ...]\n",
            "  T#71(predict/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_12/depthwise/depthwise;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise) shape:[1, 3, 3, 576], type:INT8 RO 5184 bytes, buffer: 72, data:[&, C, ., D, ., ...]\n",
            "  T#72(predict/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise) shape:[1, 3, 3, 576], type:INT8 RO 5184 bytes, buffer: 73, data:[6, J, 3, ., 5, ...]\n",
            "  T#73(predict/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_14/depthwise/depthwise;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise) shape:[1, 3, 3, 960], type:INT8 RO 8640 bytes, buffer: 74, data:[&, %, ., ., ., ...]\n",
            "  T#74(predict/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_15/depthwise/depthwise;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise) shape:[1, 3, 3, 960], type:INT8 RO 8640 bytes, buffer: 75, data:[., F, G, ., h, ...]\n",
            "  T#75(predict/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise) shape:[1, 3, 3, 960], type:INT8 RO 8640 bytes, buffer: 76, data:[., z, *, ., }, ...]\n",
            "  T#76(predict/MobilenetV2/expanded_conv_1/expand/Conv2D) shape:[96, 1, 1, 16], type:INT8 RO 1536 bytes, buffer: 77, data:[., ., ., ., ., ...]\n",
            "  T#77(predict/MobilenetV2/expanded_conv_1/project/Conv2D) shape:[24, 1, 1, 96], type:INT8 RO 2304 bytes, buffer: 78, data:[', ., ., ., *, ...]\n",
            "  T#78(predict/MobilenetV2/expanded_conv_2/expand/Conv2D) shape:[144, 1, 1, 24], type:INT8 RO 3456 bytes, buffer: 79, data:[., 8, ., ., ., ...]\n",
            "  T#79(predict/MobilenetV2/expanded_conv_2/project/Conv2D) shape:[24, 1, 1, 144], type:INT8 RO 3456 bytes, buffer: 80, data:[., \n",
            ", ., ., ., ...]\n",
            "  T#80(predict/MobilenetV2/expanded_conv_3/expand/Conv2D) shape:[144, 1, 1, 24], type:INT8 RO 3456 bytes, buffer: 81, data:[<, ., ., ., ., ...]\n",
            "  T#81(predict/MobilenetV2/expanded_conv_3/project/Conv2D) shape:[32, 1, 1, 144], type:INT8 RO 4608 bytes, buffer: 82, data:[., ., 3, d, ., ...]\n",
            "  T#82(predict/MobilenetV2/expanded_conv_4/expand/Conv2D) shape:[192, 1, 1, 32], type:INT8 RO 6144 bytes, buffer: 83, data:[(, ., ., ., ., ...]\n",
            "  T#83(predict/MobilenetV2/expanded_conv_4/project/Conv2D) shape:[32, 1, 1, 192], type:INT8 RO 6144 bytes, buffer: 84, data:[., ., ., ., ., ...]\n",
            "  T#84(predict/MobilenetV2/expanded_conv_5/expand/Conv2D) shape:[192, 1, 1, 32], type:INT8 RO 6144 bytes, buffer: 85, data:[(, ., F, p, ., ...]\n",
            "  T#85(predict/MobilenetV2/expanded_conv_5/project/Conv2D) shape:[32, 1, 1, 192], type:INT8 RO 6144 bytes, buffer: 86, data:[., ., ., ., ., ...]\n",
            "  T#86(predict/MobilenetV2/expanded_conv_6/expand/Conv2D) shape:[192, 1, 1, 32], type:INT8 RO 6144 bytes, buffer: 87, data:[., ., &, ., ., ...]\n",
            "  T#87(predict/MobilenetV2/expanded_conv_6/project/Conv2D) shape:[64, 1, 1, 192], type:INT8 RO 12288 bytes, buffer: 88, data:[\\, ., ., ., ., ...]\n",
            "  T#88(predict/MobilenetV2/expanded_conv_7/expand/Conv2D) shape:[384, 1, 1, 64], type:INT8 RO 24576 bytes, buffer: 89, data:[., ., 8, ., ., ...]\n",
            "  T#89(predict/MobilenetV2/expanded_conv_7/project/Conv2D) shape:[64, 1, 1, 384], type:INT8 RO 24576 bytes, buffer: 90, data:[C, ., ., ., ., ...]\n",
            "  T#90(predict/MobilenetV2/expanded_conv_8/expand/Conv2D) shape:[384, 1, 1, 64], type:INT8 RO 24576 bytes, buffer: 91, data:[., ., -, ., ., ...]\n",
            "  T#91(predict/MobilenetV2/expanded_conv_8/project/Conv2D) shape:[64, 1, 1, 384], type:INT8 RO 24576 bytes, buffer: 92, data:[., ., ., <, ., ...]\n",
            "  T#92(predict/MobilenetV2/expanded_conv_9/expand/Conv2D) shape:[384, 1, 1, 64], type:INT8 RO 24576 bytes, buffer: 93, data:[., ., ., ., ., ...]\n",
            "  T#93(predict/MobilenetV2/expanded_conv_9/project/Conv2D) shape:[64, 1, 1, 384], type:INT8 RO 24576 bytes, buffer: 94, data:[., ., ., ., ., ...]\n",
            "  T#94(predict/MobilenetV2/expanded_conv_10/expand/Conv2D) shape:[384, 1, 1, 64], type:INT8 RO 24576 bytes, buffer: 95, data:[., ., ., ., ., ...]\n",
            "  T#95(predict/MobilenetV2/expanded_conv_10/project/Conv2D) shape:[96, 1, 1, 384], type:INT8 RO 36864 bytes, buffer: 96, data:[., ., ., %, ., ...]\n",
            "  T#96(predict/MobilenetV2/expanded_conv_11/expand/Conv2D) shape:[576, 1, 1, 96], type:INT8 RO 55296 bytes, buffer: 97, data:[., ., ., ., ., ...]\n",
            "  T#97(predict/MobilenetV2/expanded_conv_11/project/Conv2D) shape:[96, 1, 1, 576], type:INT8 RO 55296 bytes, buffer: 98, data:[*, ., ., ., ., ...]\n",
            "  T#98(predict/MobilenetV2/expanded_conv_12/expand/Conv2D) shape:[576, 1, 1, 96], type:INT8 RO 55296 bytes, buffer: 99, data:[., ., G, ., B, ...]\n",
            "  T#99(predict/MobilenetV2/expanded_conv_12/project/Conv2D) shape:[96, 1, 1, 576], type:INT8 RO 55296 bytes, buffer: 100, data:[., \", ., ., ., ...]\n",
            "  T#100(predict/MobilenetV2/expanded_conv_13/expand/Conv2D) shape:[576, 1, 1, 96], type:INT8 RO 55296 bytes, buffer: 101, data:[., ., ., ., ., ...]\n",
            "  T#101(predict/MobilenetV2/expanded_conv_13/project/Conv2D) shape:[160, 1, 1, 576], type:INT8 RO 92160 bytes, buffer: 102, data:[., ., ., ., ., ...]\n",
            "  T#102(predict/MobilenetV2/expanded_conv_14/expand/Conv2D) shape:[960, 1, 1, 160], type:INT8 RO 153600 bytes, buffer: 103, data:[., ., ., ., ., ...]\n",
            "  T#103(predict/MobilenetV2/expanded_conv_14/project/Conv2D) shape:[160, 1, 1, 960], type:INT8 RO 153600 bytes, buffer: 104, data:[., ., ., ., ., ...]\n",
            "  T#104(predict/MobilenetV2/expanded_conv_15/expand/Conv2D) shape:[960, 1, 1, 160], type:INT8 RO 153600 bytes, buffer: 105, data:[., z, ., ., ., ...]\n",
            "  T#105(predict/MobilenetV2/expanded_conv_15/project/Conv2D) shape:[160, 1, 1, 960], type:INT8 RO 153600 bytes, buffer: 106, data:[., ., ., ., ., ...]\n",
            "  T#106(predict/MobilenetV2/expanded_conv_16/expand/Conv2D) shape:[960, 1, 1, 160], type:INT8 RO 153600 bytes, buffer: 107, data:[., N, ., ., ., ...]\n",
            "  T#107(predict/MobilenetV2/expanded_conv_16/project/Conv2D) shape:[320, 1, 1, 960], type:INT8 RO 307200 bytes, buffer: 108, data:[., ., ., ., ., ...]\n",
            "  T#108(predict/MobilenetV2/Conv_1/Conv2D) shape:[1280, 1, 1, 320], type:INT8 RO 409600 bytes, buffer: 109, data:[., ., ), ., ., ...]\n",
            "  T#109(sequential_10/dense_10/MatMul) shape:[10, 1280], type:INT8 RO 12800 bytes, buffer: 110, data:[., ., ., ., ., ...]\n",
            "  T#110(predict/hub_input/Mul) shape_signature:[-1, 224, 224, 3], type:FLOAT32\n",
            "  T#111(predict/hub_input/Sub) shape_signature:[-1, 224, 224, 3], type:FLOAT32\n",
            "  T#112(predict/MobilenetV2/Conv/Relu6;predict/MobilenetV2/Conv/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_5/project/Conv2D;predict/MobilenetV2/Conv/Conv2D) shape_signature:[-1, 112, 112, 32], type:FLOAT32\n",
            "  T#113(predict/MobilenetV2/expanded_conv/depthwise/Relu6;predict/MobilenetV2/expanded_conv/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv/depthwise/depthwise;predict/MobilenetV2/expanded_conv_5/project/Conv2D) shape_signature:[-1, 112, 112, 32], type:FLOAT32\n",
            "  T#114(predict/MobilenetV2/expanded_conv/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv/project/Conv2D) shape_signature:[-1, 112, 112, 16], type:FLOAT32\n",
            "  T#115(predict/MobilenetV2/expanded_conv_1/expand/Relu6;predict/MobilenetV2/expanded_conv_1/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_12/project/Conv2D;predict/MobilenetV2/expanded_conv_1/expand/Conv2D) shape_signature:[-1, 112, 112, 96], type:FLOAT32\n",
            "  T#116(predict/MobilenetV2/expanded_conv_1/depthwise/Relu6;predict/MobilenetV2/expanded_conv_1/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_12/project/Conv2D;predict/MobilenetV2/expanded_conv_1/depthwise/depthwise) shape_signature:[-1, 56, 56, 96], type:FLOAT32\n",
            "  T#117(predict/MobilenetV2/expanded_conv_1/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_2/project/Conv2D;predict/MobilenetV2/expanded_conv_1/project/Conv2D) shape_signature:[-1, 56, 56, 24], type:FLOAT32\n",
            "  T#118(predict/MobilenetV2/expanded_conv_2/expand/Relu6;predict/MobilenetV2/expanded_conv_2/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_3/depthwise/depthwise;predict/MobilenetV2/expanded_conv_2/expand/Conv2D) shape_signature:[-1, 56, 56, 144], type:FLOAT32\n",
            "  T#119(predict/MobilenetV2/expanded_conv_2/depthwise/Relu6;predict/MobilenetV2/expanded_conv_2/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_3/depthwise/depthwise;predict/MobilenetV2/expanded_conv_2/depthwise/depthwise) shape_signature:[-1, 56, 56, 144], type:FLOAT32\n",
            "  T#120(predict/MobilenetV2/expanded_conv_2/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_2/project/Conv2D) shape_signature:[-1, 56, 56, 24], type:FLOAT32\n",
            "  T#121(predict/MobilenetV2/expanded_conv_2/add) shape_signature:[-1, 56, 56, 24], type:FLOAT32\n",
            "  T#122(predict/MobilenetV2/expanded_conv_3/expand/Relu6;predict/MobilenetV2/expanded_conv_3/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_3/depthwise/depthwise;predict/MobilenetV2/expanded_conv_3/expand/Conv2D) shape_signature:[-1, 56, 56, 144], type:FLOAT32\n",
            "  T#123(predict/MobilenetV2/expanded_conv_3/depthwise/Relu6;predict/MobilenetV2/expanded_conv_3/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_3/depthwise/depthwise) shape_signature:[-1, 28, 28, 144], type:FLOAT32\n",
            "  T#124(predict/MobilenetV2/expanded_conv_3/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_5/project/Conv2D;predict/MobilenetV2/expanded_conv_3/project/Conv2D) shape_signature:[-1, 28, 28, 32], type:FLOAT32\n",
            "  T#125(predict/MobilenetV2/expanded_conv_4/expand/Relu6;predict/MobilenetV2/expanded_conv_4/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise;predict/MobilenetV2/expanded_conv_4/expand/Conv2D) shape_signature:[-1, 28, 28, 192], type:FLOAT32\n",
            "  T#126(predict/MobilenetV2/expanded_conv_4/depthwise/Relu6;predict/MobilenetV2/expanded_conv_4/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise;predict/MobilenetV2/expanded_conv_4/depthwise/depthwise) shape_signature:[-1, 28, 28, 192], type:FLOAT32\n",
            "  T#127(predict/MobilenetV2/expanded_conv_4/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_5/project/Conv2D;predict/MobilenetV2/expanded_conv_4/project/Conv2D) shape_signature:[-1, 28, 28, 32], type:FLOAT32\n",
            "  T#128(predict/MobilenetV2/expanded_conv_4/add) shape_signature:[-1, 28, 28, 32], type:FLOAT32\n",
            "  T#129(predict/MobilenetV2/expanded_conv_5/expand/Relu6;predict/MobilenetV2/expanded_conv_5/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise;predict/MobilenetV2/expanded_conv_5/expand/Conv2D) shape_signature:[-1, 28, 28, 192], type:FLOAT32\n",
            "  T#130(predict/MobilenetV2/expanded_conv_5/depthwise/Relu6;predict/MobilenetV2/expanded_conv_5/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise;predict/MobilenetV2/expanded_conv_5/depthwise/depthwise) shape_signature:[-1, 28, 28, 192], type:FLOAT32\n",
            "  T#131(predict/MobilenetV2/expanded_conv_5/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_5/project/Conv2D) shape_signature:[-1, 28, 28, 32], type:FLOAT32\n",
            "  T#132(predict/MobilenetV2/expanded_conv_5/add) shape_signature:[-1, 28, 28, 32], type:FLOAT32\n",
            "  T#133(predict/MobilenetV2/expanded_conv_6/expand/Relu6;predict/MobilenetV2/expanded_conv_6/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise;predict/MobilenetV2/expanded_conv_6/expand/Conv2D) shape_signature:[-1, 28, 28, 192], type:FLOAT32\n",
            "  T#134(predict/MobilenetV2/expanded_conv_6/depthwise/Relu6;predict/MobilenetV2/expanded_conv_6/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_6/depthwise/depthwise) shape_signature:[-1, 14, 14, 192], type:FLOAT32\n",
            "  T#135(predict/MobilenetV2/expanded_conv_6/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_9/project/Conv2D;predict/MobilenetV2/expanded_conv_6/project/Conv2D) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#136(predict/MobilenetV2/expanded_conv_7/expand/Relu6;predict/MobilenetV2/expanded_conv_7/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_7/expand/Conv2D) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#137(predict/MobilenetV2/expanded_conv_7/depthwise/Relu6;predict/MobilenetV2/expanded_conv_7/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_7/depthwise/depthwise) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#138(predict/MobilenetV2/expanded_conv_7/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_9/project/Conv2D;predict/MobilenetV2/expanded_conv_7/project/Conv2D) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#139(predict/MobilenetV2/expanded_conv_7/add) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#140(predict/MobilenetV2/expanded_conv_8/expand/Relu6;predict/MobilenetV2/expanded_conv_8/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_8/expand/Conv2D) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#141(predict/MobilenetV2/expanded_conv_8/depthwise/Relu6;predict/MobilenetV2/expanded_conv_8/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_8/depthwise/depthwise) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#142(predict/MobilenetV2/expanded_conv_8/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_9/project/Conv2D;predict/MobilenetV2/expanded_conv_8/project/Conv2D) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#143(predict/MobilenetV2/expanded_conv_8/add) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#144(predict/MobilenetV2/expanded_conv_9/expand/Relu6;predict/MobilenetV2/expanded_conv_9/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_9/expand/Conv2D) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#145(predict/MobilenetV2/expanded_conv_9/depthwise/Relu6;predict/MobilenetV2/expanded_conv_9/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_9/depthwise/depthwise) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#146(predict/MobilenetV2/expanded_conv_9/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_9/project/Conv2D) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#147(predict/MobilenetV2/expanded_conv_9/add) shape_signature:[-1, 14, 14, 64], type:FLOAT32\n",
            "  T#148(predict/MobilenetV2/expanded_conv_10/expand/Relu6;predict/MobilenetV2/expanded_conv_10/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise;predict/MobilenetV2/expanded_conv_10/expand/Conv2D) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#149(predict/MobilenetV2/expanded_conv_10/depthwise/Relu6;predict/MobilenetV2/expanded_conv_10/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_10/depthwise/depthwise) shape_signature:[-1, 14, 14, 384], type:FLOAT32\n",
            "  T#150(predict/MobilenetV2/expanded_conv_10/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_12/project/Conv2D;predict/MobilenetV2/expanded_conv_10/project/Conv2D) shape_signature:[-1, 14, 14, 96], type:FLOAT32\n",
            "  T#151(predict/MobilenetV2/expanded_conv_11/expand/Relu6;predict/MobilenetV2/expanded_conv_11/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise;predict/MobilenetV2/expanded_conv_11/expand/Conv2D) shape_signature:[-1, 14, 14, 576], type:FLOAT32\n",
            "  T#152(predict/MobilenetV2/expanded_conv_11/depthwise/Relu6;predict/MobilenetV2/expanded_conv_11/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise;predict/MobilenetV2/expanded_conv_11/depthwise/depthwise) shape_signature:[-1, 14, 14, 576], type:FLOAT32\n",
            "  T#153(predict/MobilenetV2/expanded_conv_11/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_12/project/Conv2D;predict/MobilenetV2/expanded_conv_11/project/Conv2D) shape_signature:[-1, 14, 14, 96], type:FLOAT32\n",
            "  T#154(predict/MobilenetV2/expanded_conv_11/add) shape_signature:[-1, 14, 14, 96], type:FLOAT32\n",
            "  T#155(predict/MobilenetV2/expanded_conv_12/expand/Relu6;predict/MobilenetV2/expanded_conv_12/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise;predict/MobilenetV2/expanded_conv_12/expand/Conv2D) shape_signature:[-1, 14, 14, 576], type:FLOAT32\n",
            "  T#156(predict/MobilenetV2/expanded_conv_12/depthwise/Relu6;predict/MobilenetV2/expanded_conv_12/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise;predict/MobilenetV2/expanded_conv_12/depthwise/depthwise) shape_signature:[-1, 14, 14, 576], type:FLOAT32\n",
            "  T#157(predict/MobilenetV2/expanded_conv_12/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_12/project/Conv2D) shape_signature:[-1, 14, 14, 96], type:FLOAT32\n",
            "  T#158(predict/MobilenetV2/expanded_conv_12/add) shape_signature:[-1, 14, 14, 96], type:FLOAT32\n",
            "  T#159(predict/MobilenetV2/expanded_conv_13/expand/Relu6;predict/MobilenetV2/expanded_conv_13/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise;predict/MobilenetV2/expanded_conv_13/expand/Conv2D) shape_signature:[-1, 14, 14, 576], type:FLOAT32\n",
            "  T#160(predict/MobilenetV2/expanded_conv_13/depthwise/Relu6;predict/MobilenetV2/expanded_conv_13/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_13/depthwise/depthwise) shape_signature:[-1, 7, 7, 576], type:FLOAT32\n",
            "  T#161(predict/MobilenetV2/expanded_conv_13/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_15/project/Conv2D;predict/MobilenetV2/expanded_conv_13/project/Conv2D) shape_signature:[-1, 7, 7, 160], type:FLOAT32\n",
            "  T#162(predict/MobilenetV2/expanded_conv_14/expand/Relu6;predict/MobilenetV2/expanded_conv_14/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise;predict/MobilenetV2/expanded_conv_14/expand/Conv2D) shape_signature:[-1, 7, 7, 960], type:FLOAT32\n",
            "  T#163(predict/MobilenetV2/expanded_conv_14/depthwise/Relu6;predict/MobilenetV2/expanded_conv_14/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise;predict/MobilenetV2/expanded_conv_14/depthwise/depthwise) shape_signature:[-1, 7, 7, 960], type:FLOAT32\n",
            "  T#164(predict/MobilenetV2/expanded_conv_14/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_15/project/Conv2D;predict/MobilenetV2/expanded_conv_14/project/Conv2D) shape_signature:[-1, 7, 7, 160], type:FLOAT32\n",
            "  T#165(predict/MobilenetV2/expanded_conv_14/add) shape_signature:[-1, 7, 7, 160], type:FLOAT32\n",
            "  T#166(predict/MobilenetV2/expanded_conv_15/expand/Relu6;predict/MobilenetV2/expanded_conv_15/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise;predict/MobilenetV2/expanded_conv_15/expand/Conv2D) shape_signature:[-1, 7, 7, 960], type:FLOAT32\n",
            "  T#167(predict/MobilenetV2/expanded_conv_15/depthwise/Relu6;predict/MobilenetV2/expanded_conv_15/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise;predict/MobilenetV2/expanded_conv_15/depthwise/depthwise) shape_signature:[-1, 7, 7, 960], type:FLOAT32\n",
            "  T#168(predict/MobilenetV2/expanded_conv_15/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_15/project/Conv2D) shape_signature:[-1, 7, 7, 160], type:FLOAT32\n",
            "  T#169(predict/MobilenetV2/expanded_conv_15/add) shape_signature:[-1, 7, 7, 160], type:FLOAT32\n",
            "  T#170(predict/MobilenetV2/expanded_conv_16/expand/Relu6;predict/MobilenetV2/expanded_conv_16/expand/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise;predict/MobilenetV2/expanded_conv_16/expand/Conv2D) shape_signature:[-1, 7, 7, 960], type:FLOAT32\n",
            "  T#171(predict/MobilenetV2/expanded_conv_16/depthwise/Relu6;predict/MobilenetV2/expanded_conv_16/depthwise/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/depthwise/depthwise) shape_signature:[-1, 7, 7, 960], type:FLOAT32\n",
            "  T#172(predict/MobilenetV2/expanded_conv_16/project/BatchNorm/FusedBatchNorm;predict/MobilenetV2/expanded_conv_16/project/Conv2D) shape_signature:[-1, 7, 7, 320], type:FLOAT32\n",
            "  T#173(predict/MobilenetV2/Conv_1/Relu6;predict/MobilenetV2/Conv_1/BatchNorm/FusedBatchNorm;predict/MobilenetV2/Conv_1/Conv2D) shape_signature:[-1, 7, 7, 1280], type:FLOAT32\n",
            "  T#174(predict/MobilenetV2/Logits/AvgPool) shape_signature:[-1, 1, 1, 1280], type:FLOAT32\n",
            "  T#175(predict/feature_vector/SpatialSqueeze1) shape_signature:[-1, 1280], type:FLOAT32\n",
            "  T#176(sequential_10/dense_10/MatMul;sequential_10/dense_10/BiasAdd) shape_signature:[-1, 10], type:FLOAT32\n",
            "  T#177(StatefulPartitionedCall:0) shape_signature:[-1, 10], type:FLOAT32\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Your TFLite model has '1' signature_def(s).\n",
            "\n",
            "Signature#0 key: 'serving_default'\n",
            "- Subgraph: Subgraph#0\n",
            "- Inputs: \n",
            "    'keras_layer_8_input' : T#0\n",
            "- Outputs: \n",
            "    'dense_10' : T#177\n",
            "\n",
            "---------------------------------------------------------------\n",
            "              Model size:    2520056 bytes\n",
            "    Non-data buffer size:     241528 bytes (09.58 %)\n",
            "  Total data buffer size:    2278528 bytes (90.42 %)\n",
            "    (Zero value buffers):          0 bytes (00.00 %)\n",
            "\n",
            "* Buffers of TFLite model are mostly used for constant tensors.\n",
            "  And zero value buffers are buffers filled with zeros.\n",
            "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
            "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# 2-4.전이학습+최적화(float fallback quantization)\n",
        "# + tflite로 변환\n",
        "# weights and variable data are quantized\n",
        "################################\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "print(metadata)\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "#텐서플로우 허브에 있는 mobilenet_v2 모델을 사용해 feature_extractor라는 케라스 층을 만듭니다.\n",
        "#신경망의 첫번째 층\n",
        "handle_base, pixels, FV_SIZE = (\"mobilenet_v2\", 224, 1280)\n",
        "\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "\n",
        "IMAGE_SIZE =(pixels, pixels)\n",
        "\n",
        "\n",
        "feature_extractor = hub.KerasLayer(\n",
        "    MODULE_HANDLE, #모델\n",
        "    input_shape = IMAGE_SIZE +(3,),# #(224,224,3) Expects a tensor of shape [batch_size] as input.\n",
        "    output_shape = [FV_SIZE], # Outputs a tensor with shape [batch_size, 20].\n",
        "    trainable = False\n",
        ")\n",
        "\n",
        "# 모델 선언\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dense(num_classes, activation ='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "kmnist_SAVED_MODEL = '/content/drive/MyDrive/INTEL_PYTHON/exp_saved_model'\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "#########################\n",
        "# 대표 샘플을 뽑아서, variable data 양자화\n",
        "############################\n",
        "def representative_data_gen():\n",
        "  for input_value, _ in test_batches.take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tflite_model_ffq = converter.convert()\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/ffq_converted_model.tflite'\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model_ffq)\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_ffq)\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "print('input: ', input_details)\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "print('output: ', output_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuev2Sf2NaXD",
        "outputId": "610e0508-9ae8-401b-edbd-e7509c34ec3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='kmnist',\n",
            "    full_name='kmnist/3.0.1',\n",
            "    description=\"\"\"\n",
            "    Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images), provided in the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes, we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.\n",
            "    \"\"\",\n",
            "    homepage='http://codh.rois.ac.jp/kmnist/index.html.en',\n",
            "    data_dir='/root/tensorflow_datasets/kmnist/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=20.26 MiB,\n",
            "    dataset_size=31.76 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@online{clanuwat2018deep,\n",
            "      author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha},\n",
            "      title        = {Deep Learning for Classical Japanese Literature},\n",
            "      date         = {2018-12-03},\n",
            "      year         = {2018},\n",
            "      eprintclass  = {cs.CV},\n",
            "      eprinttype   = {arXiv},\n",
            "      eprint       = {cs.CV/1812.01718},\n",
            "    }\"\"\",\n",
            ")\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 70s 43ms/step - loss: 0.5821 - accuracy: 0.8255 - val_loss: 0.3527 - val_accuracy: 0.8990\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 64s 43ms/step - loss: 0.3148 - accuracy: 0.9077 - val_loss: 0.2956 - val_accuracy: 0.9102\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 64s 43ms/step - loss: 0.2572 - accuracy: 0.9234 - val_loss: 0.2611 - val_accuracy: 0.9193\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 60s 40ms/step - loss: 0.2249 - accuracy: 0.9327 - val_loss: 0.2438 - val_accuracy: 0.9250\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 60s 40ms/step - loss: 0.2038 - accuracy: 0.9389 - val_loss: 0.2374 - val_accuracy: 0.9258\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_9 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2270794 (8.66 MB)\n",
            "Trainable params: 12810 (50.04 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n",
            "input:  {'name': 'serving_default_keras_layer_9_input:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "output:  {'name': 'StatefulPartitionedCall:0', 'index': 179, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# 2-4.전이학습+최적화(완전한 정수 양자화 (full integer quantization))\n",
        "# + tflite로 변환\n",
        "# weights and variable data are quantized\n",
        "################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "print(metadata)\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "#텐서플로우 허브에 있는 mobilenet_v2 모델을 사용해 feature_extractor라는 케라스 층을 만듭니다.\n",
        "#신경망의 첫번째 층\n",
        "handle_base, pixels, FV_SIZE = (\"mobilenet_v2\", 224, 1280)\n",
        "\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "\n",
        "IMAGE_SIZE =(pixels, pixels)\n",
        "\n",
        "\n",
        "feature_extractor = hub.KerasLayer(\n",
        "    MODULE_HANDLE, #모델\n",
        "    input_shape = IMAGE_SIZE +(3,),# #(224,224,3) Expects a tensor of shape [batch_size] as input.\n",
        "    output_shape = [FV_SIZE], # Outputs a tensor with shape [batch_size, 20].\n",
        "    trainable = False\n",
        ")\n",
        "\n",
        "# 모델 선언\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dense(num_classes, activation ='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "kmnist_SAVED_MODEL = '/content/drive/MyDrive/INTEL_PYTHON/exp_saved_model'\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "#tensoflow lite로 변환하기\n",
        "def representative_data_gen():\n",
        "  for input_value, _ in test_batches.take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_fiq = converter.convert()\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/fiq_converted_model.tflite'\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model_fiq)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_fiq)\n",
        "input_details = interpreter.get_input_details()[0]\n",
        "print('input: ', input_details)\n",
        "output_details = interpreter.get_output_details()[0]\n",
        "print('output: ', output_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ji_PFAcOgHB",
        "outputId": "2ed4388c-fe0e-4791-d8ac-8dee4b0ca5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='kmnist',\n",
            "    full_name='kmnist/3.0.1',\n",
            "    description=\"\"\"\n",
            "    Kuzushiji-MNIST is a drop-in replacement for the MNIST dataset (28x28 grayscale, 70,000 images), provided in the original MNIST format as well as a NumPy format. Since MNIST restricts us to 10 classes, we chose one character to represent each of the 10 rows of Hiragana when creating Kuzushiji-MNIST.\n",
            "    \"\"\",\n",
            "    homepage='http://codh.rois.ac.jp/kmnist/index.html.en',\n",
            "    data_dir='/root/tensorflow_datasets/kmnist/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=20.26 MiB,\n",
            "    dataset_size=31.76 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@online{clanuwat2018deep,\n",
            "      author       = {Tarin Clanuwat and Mikel Bober-Irizar and Asanobu Kitamoto and Alex Lamb and Kazuaki Yamamoto and David Ha},\n",
            "      title        = {Deep Learning for Classical Japanese Literature},\n",
            "      date         = {2018-12-03},\n",
            "      year         = {2018},\n",
            "      eprintclass  = {cs.CV},\n",
            "      eprinttype   = {arXiv},\n",
            "      eprint       = {cs.CV/1812.01718},\n",
            "    }\"\"\",\n",
            ")\n",
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 74s 47ms/step - loss: 0.5790 - accuracy: 0.8278 - val_loss: 0.3526 - val_accuracy: 0.8940\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 64s 43ms/step - loss: 0.3153 - accuracy: 0.9079 - val_loss: 0.2851 - val_accuracy: 0.9142\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2563 - accuracy: 0.9250 - val_loss: 0.2599 - val_accuracy: 0.9252\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 64s 42ms/step - loss: 0.2247 - accuracy: 0.9321 - val_loss: 0.2393 - val_accuracy: 0.9287\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2027 - accuracy: 0.9391 - val_loss: 0.2351 - val_accuracy: 0.9280\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_10 (KerasLayer  (None, 1280)              2257984   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2270794 (8.66 MB)\n",
            "Trainable params: 12810 (50.04 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n",
            "input:  {'name': 'serving_default_keras_layer_10_input:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0), 'quantization_parameters': {'scales': array([0.00392157], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "output:  {'name': 'StatefulPartitionedCall:0', 'index': 179, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([0], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# 2-3.전이학습+최적화(float16 quantization) + tflite로 변환\n",
        "################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#244x 244 사이즈로 리사이즈를 하고, 정규화를 해줍니다.\n",
        "def format_image(image,label):\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.resize(image,(224,224))/255.0\n",
        "    return image, label\n",
        "\n",
        "#훈련, 검증, 테스트 세트로 나눕니다.\n",
        "(raw_train, raw_validation, raw_test), metadata =tfds.load(\n",
        "    'kmnist',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples//4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "test_batches = raw_test.map(format_image).batch(1)\n",
        "\n",
        "#텐서플로우 허브에 있는 mobilenet_v2 모델을 사용해 feature_extractor라는 케라스 층을 만듭니다.\n",
        "#신경망의 첫번째 층\n",
        "handle_base, pixels, FV_SIZE = (\"mobilenet_v2\", 224, 1280)\n",
        "\n",
        "MODULE_HANDLE = \"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "\n",
        "IMAGE_SIZE =(pixels, pixels)\n",
        "\n",
        "\n",
        "feature_extractor = hub.KerasLayer(\n",
        "    MODULE_HANDLE, #모델\n",
        "    input_shape = IMAGE_SIZE +(3,), #(224,224,3)Expects a tensor of shape [batch_size] as input.\n",
        "    output_shape = [FV_SIZE], # # Outputs a tensor with shape [batch_size, 20].\n",
        "    trainable = False\n",
        ")\n",
        "\n",
        "# 모델 선언\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor,\n",
        "    tf.keras.layers.Dense(num_classes, activation ='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss ='sparse_categorical_crossentropy',\n",
        "    metrics =['accuracy']\n",
        ")\n",
        "\n",
        "hist = model.fit(\n",
        "    train_batches,\n",
        "    epochs = 5,\n",
        "    validation_data = validation_batches\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#학습된 모델 저장하기\n",
        "\n",
        "CATS_VS_DOGS_SAVED_MODEL = '/content/drive/MyDrive/INTEL_PYTHON/exp_saved_model'\n",
        "tf.saved_model.save(model, kmnist_SAVED_MODEL)\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(kmnist_SAVED_MODEL)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types=[tf.float16]\n",
        "\n",
        "tflite_model_ffq = converter.convert()\n",
        "tflite_model_file = '/content/drive/MyDrive/INTEL_PYTHON/f16q_converted_model.tflite'\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model_ffq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8eVCY67QNaX",
        "outputId": "c3c159c7-376e-4d91-c13b-799f7dc97a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 68s 42ms/step - loss: 0.5697 - accuracy: 0.8325 - val_loss: 0.3480 - val_accuracy: 0.8968\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 64s 43ms/step - loss: 0.3130 - accuracy: 0.9080 - val_loss: 0.2882 - val_accuracy: 0.9132\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 64s 42ms/step - loss: 0.2550 - accuracy: 0.9251 - val_loss: 0.2494 - val_accuracy: 0.9227\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 59s 39ms/step - loss: 0.2239 - accuracy: 0.9328 - val_loss: 0.2485 - val_accuracy: 0.9247\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 59s 39ms/step - loss: 0.2023 - accuracy: 0.9402 - val_loss: 0.2285 - val_accuracy: 0.9297\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_11 (KerasLayer  (None, 1280)              2257984   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2270794 (8.66 MB)\n",
            "Trainable params: 12810 (50.04 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}